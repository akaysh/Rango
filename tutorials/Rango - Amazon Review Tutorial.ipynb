{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.1, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using  Languages, TextAnalysis, Flux, PyPlot, Statistics, MLDataUtils, Embeddings, MLLabelUtils\n",
    "\n",
    "# function to return the index of the word in the word dictionary\n",
    "tk_idx(s) = haskey(word_dict, s) ? i=word_dict[s] : i=0\n",
    "\n",
    "# Padding the corpus wrt the longest document\n",
    "function pad_corpus(c, pad_size)\n",
    "    M=[]\n",
    "    for doc in 1:length(c)\n",
    "        tks = tokens(c[doc])\n",
    "        if length(tks)>=pad_size\n",
    "            tk_indexes=[tk_idx(w) for w in tks[1:pad_size]]\n",
    "        end\n",
    "        if length(tks)<pad_size\n",
    "            tk_indexes=zeros(Int64,pad_size-length(tks))\n",
    "            tk_indexes=vcat(tk_indexes, [tk_idx(w) for w in tks])\n",
    "        end\n",
    "        doc==1 ? M=tk_indexes' : M=vcat(M, tk_indexes')\n",
    "    end\n",
    "    return M\n",
    "end\n",
    "\n",
    "accuracy(x, y, model) = mean(Flux.onecold(model(x)) .== Flux.onecold(y))\n",
    "\n",
    "loss(x, y) = sum(Flux.binarycrossentropy.(m(x), y))\n",
    "\n",
    "# map_binary_encoding(labels) = [label == \"__label__1\" ? 0 : 1 for label in labels ] \n",
    "optimizer = ADAM(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/amazon_reviews.txt\"\n",
    "τ = 500 # max Iterations\n",
    "# M = 50 # max_features\n",
    "\n",
    "f = open(data_path)\n",
    "doc_array = readlines(f)\n",
    "\n",
    "labels, texts = [], []\n",
    "for doc in doc_array\n",
    "    content = split(doc)\n",
    "    push!(labels,content[1])\n",
    "    push!(texts,join(content[2:end],\" \"))\n",
    "end\n",
    "\n",
    "# pushing the text from the files to the string documents\n",
    "docs=[]\n",
    "for i in 1:length(texts)\n",
    "    push!(docs, StringDocument(texts[i]))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_embedding (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const embtable = load_embeddings(GloVe{:en},4) # or load_embeddings(FastText_Text) or ...\n",
    "#Function to return the index of the word in the embedding (returns 0 if the word is not found)\n",
    "const get_word_index = Dict(word=>ii for (ii,word) in enumerate(embtable.vocab))\n",
    "\n",
    "function get_embedding(word)\n",
    "    ind = get_word_index[word]\n",
    "    emb = embtable.embeddings[:,ind]\n",
    "    return emb\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 400000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embtable.embeddings\n",
    "vocab = embtable.vocab\n",
    "embed_size, max_features = size(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building Flux Embeddings\n",
    "max_features = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54077"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a Corpus\n",
    "corpus=Corpus(docs)\n",
    "\n",
    "# updating the lexicon and creating the word dict\n",
    "update_lexicon!(corpus)\n",
    "doc_term_matrix=DocumentTermMatrix(corpus)\n",
    "word_dict = doc_term_matrix.column_indices\n",
    "# splitting words in the document\n",
    "word_docs = map(s -> split(s,r\"[,. ]\",keepempty=false),texts)\n",
    "# pad size is the number of words in the maximum word document\n",
    "\n",
    "# Can set a fixed length or the max doc length\n",
    "# pad_size = maximum(length(word_docs[i]) for i in 1:length(texts)) \n",
    "pad_size = 70\n",
    "\n",
    "# padding the docs\n",
    "padded_docs = pad_corpus(corpus, pad_size)\n",
    "# forming the data with the labels\n",
    "x = padded_docs'\n",
    "# train_indices = [1:3:150 ; 2:3:150]\n",
    "# X_train = x[:, train_indices]\n",
    "# y_train = labels[:, train_indices]\n",
    "# X_test = x[:, 3:3:150]\n",
    "# y_test = labels[:, 3:3:150]\n",
    "(X_train, y_train), (X_test, y_test) = splitobs((x, labels); at = 0.67)\n",
    "# x_train = Array(transpose(X_train1))\n",
    "# y_train = Array(y_train1)\n",
    "# x_test = Array(transpose(X_test1))\n",
    "# y_test = Array(y_test1)\n",
    "klasses = sort(unique(labels))\n",
    "y_train = Flux.onehotbatch(y_train,klasses)\n",
    "data = [(X_train, y_train)]\n",
    "\n",
    "# Building Flux Embeddings\n",
    "N = size(X_train,2)  #Number of documents\n",
    "# features per word to learn, depends on the size of the corpus, larger corpus will probably need a higher dimension\n",
    "# max_features = M\n",
    "# number of words in the vocabulary, should always be higher than the maximum index in our dictionary.\n",
    "ν = maximum(word_dict)[2] + 1\n",
    "vocab_size = ν"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer for Flux model\n",
    "# glorot_normal returns an Array of size dims containing random variables taken from a normal distribution with mean 0 and standard deviation (2 / sum(dims)).\n",
    "embedding_matrix=Flux.glorot_normal(max_features, vocab_size)\n",
    "\n",
    "function vec_idx(s)\n",
    "    i=findfirst(x -> x==s, vocab)\n",
    "    i==nothing ? i=0 : i \n",
    "end\n",
    "\n",
    "\n",
    "for term in doc_term_matrix.terms\n",
    "    if vec_idx(term)!=0\n",
    "        embedding_matrix[:,word_dict[term]+1]=get_embedding(term)\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(#25, #26, #27, #28, Recur(LSTMCell(300, 100)), Dense(100, 50, relu), Dense(50, 1, σ))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enabling Flux\n",
    "\n",
    "\n",
    "\n",
    "m = Chain(x -> embedding_matrix * Flux.onehotbatch(reshape(x, pad_size*N), 0:vocab_size-1),\n",
    "          x -> reshape(x, max_features, pad_size, N),\n",
    "          x -> sum(x, dims=2),\n",
    "          x -> reshape(x, max_features, N),\n",
    "        LSTM(max_features,100),\n",
    "        Dense(100,50,relu),\n",
    "        Dense(50,1,σ)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_h=[]\n",
    "accuracy_train=[]\n",
    "\n",
    "for epoch in 1:100\n",
    "    Flux.train!(loss, Flux.params(m), data, optimizer)\n",
    "    println(loss(X_train, y_train), \" \", accuracy(X_train, y_train, m))\n",
    "    push!(loss_h, loss(X_train, y_train))\n",
    "    push!(accuracy_train, accuracy(X_train, y_train, m))\n",
    "end\n",
    "\n",
    "print(Flux.onecold(m(X_train)), accuracy(X_train, y_train, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2607×17500 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  1  0  1  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " ⋮              ⋮              ⋮        ⋱        ⋮              ⋮            \n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.onehotbatch(reshape(x, pad_size*N), 0:vocab_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 2606 entries:\n",
       "  \"1\"           => 20\n",
       "  \"Bill\"        => 123\n",
       "  \"neither.\"    => 1740\n",
       "  \"Bateman\"     => 118\n",
       "  \"Cape\"        => 146\n",
       "  \"doctor\"      => 1073\n",
       "  \"enjoy\"       => 1124\n",
       "  \"chocolate\"   => 882\n",
       "  \"fight\"       => 1225\n",
       "  \"spent\"       => 2259\n",
       "  \"regular\"     => 2021\n",
       "  \"culture.\"    => 999\n",
       "  \"artisan\"     => 703\n",
       "  \"favorites\"   => 1209\n",
       "  \"frustrating\" => 1285\n",
       "  \"loosely\"     => 1618\n",
       "  \"haze\"        => 1386\n",
       "  \"par.\"        => 1840\n",
       "  \"step\"        => 2278\n",
       "  \"Many\"        => 340\n",
       "  \"download\"    => 1088\n",
       "  \"gives\"       => 1321\n",
       "  \"irrelevant\"  => 1508\n",
       "  \"lean\"        => 1572\n",
       "  \"poised\"      => 1912\n",
       "  ⋮             => ⋮"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLLabelUtils.LabelEnc.OneOfK{Bool,2}()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klasses = sort(unique(labels))\n",
    "ll = labelenc(Flux.onehotbatch(labels,klasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Array{Int64,1}} with 2 entries:\n",
       "  \"__label__2\" => [1, 2, 3, 4, 5, 6, 8, 9, 10, 12  …  482, 483, 487, 488, 495, …\n",
       "  \"__label__1\" => [7, 11, 14, 15, 16, 20, 21, 23, 26, 27  …  480, 484, 485, 486…"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelmap(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×500 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 0  0  0  0  0  0  1  0  0  0  1  0  0  …  1  1  1  1  1  1  0  0  0  0  0  0\n",
       " 1  1  1  1  1  1  0  1  1  1  0  1  1     0  0  0  0  0  0  1  1  1  1  1  1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.onehotbatch(labels, klasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70×500 LinearAlgebra.Adjoint{Int64,Array{Int64,2}}:\n",
       " 1943  2036   214   737  1686  2511  …     0  1047  1674   885   687  1442\n",
       " 3963  2793     1  1897    13  2335        0  8043  7350  7394  1028  5502\n",
       " 4274  6965   128   128  1629  5362        0  3726  2525  2856  1484  5583\n",
       " 7350  3966  2048  1017  2308   128        0     8  2540  7467  1197  7341\n",
       " 5620  7467  6965  7578  1114  1017        0  7244  6501  2324   128  7469\n",
       "  128  2549  4968  5199  1502  2497  …   346  4572  4274  8049   705   128\n",
       " 2048   128  5550  7394  2036  6244     2856  7394  2324  3121  4810  1950\n",
       " 6961  1017  4132  6965   809  7204     5705    13  6308   128  1510    13\n",
       " 7518     8  5542  2518   194  2542      705  4404  4845   705  1248  2324\n",
       " 7828  5299  5680  1017   967  5680     4274  4978   128  4810  4968  4161\n",
       " 2744  6302  2466  3906  4978  8043  …  7350     1  1017  1510  2534  5680\n",
       "    1  2324  7444  7758   128  2382     4393   128  7388  1248  4482  7350\n",
       " 1081  5269    13  4375  1047  7256     6298  2048  2802  3865     2  6986\n",
       "    ⋮                             ⋮  ⋱           ⋮                        \n",
       " 1081  2760  4798    13  1808  5450     1017  5738  7350  2324  3388  7421\n",
       " 2691  5219  4886  1280    13  5680     5032  2036  5709  2768  3248  7350\n",
       " 2671  7467  7467   822  1335  7350  …  7939  5719  6298    13  2518  3927\n",
       " 4342  4978  5379    13   674  7917     5502  7384  7915  2732  3148  6194\n",
       " 3435  4274  5915  2518   128  4377     5680  7344  2503  7204  3380  6627\n",
       " 5045  8033  7423   678  2036  5680     5761  5042  7828  7467    13  7242\n",
       " 2518  5652  5550  2159  2258    95     6021  5379  2347  4420  7350  2518\n",
       " 7254  2518  7298  2105    13   415  …  4529  4342  7467  7394  5795     2\n",
       " 2324  4989  8034   128  2036  2518     6293  4423  7350  2856  5315  4645\n",
       " 4330  2749  2036  2036  1918  6244     7394  4978  2133  7467  5680  3839\n",
       " 7059  6820  4645   668    13  2324     2856   110  5680  8049  7375     2\n",
       " 7949  6380  3898    13  2518  4161       22  7828   478  3121  6902  7952"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70×6700 view(::LinearAlgebra.Adjoint{Int64,Array{Int64,2}}, :, 1:6700) with eltype Int64:\n",
       " 15788  16415   1531   6144  13827  …      0      0  15190    937      0\n",
       " 29004  21242      1  15454     31         0      0  31128  10754      0\n",
       " 30694  47554    903    903  13268         0      0  48458  16295      0\n",
       " 49788  29022  16463   8403  18254         0      0  44138      1      0\n",
       " 39074  50409  47554  51096   9041         0      0    903    903      0\n",
       "   903  19754  34905  36220  12018  …      0      0  16463   1973      0\n",
       " 16463    903  38572  50034  16415         0      0  34905   5524      0\n",
       " 47531   8403  29930  47554   6682         0      0  26187  49300      0\n",
       " 50708     19  38491  19524   1336         0      0  19484  49788      0\n",
       " 52809  36758  39526   8403   7985         0  11810  19938  51913      0\n",
       " 20987  43541  19238  28617  34954  …      0  51941  31293  43531      0\n",
       "     1  18357  50287  52387    903         0  50409   8859  39659      0\n",
       "  8859  36602     31  31292   8580         0   7645  32543  18357      0\n",
       "     ⋮                              ⋱      ⋮                            \n",
       "  8859  21042  33849     31  14740     39526  49232   6391  50531  23055\n",
       " 20657  36343  34424  10193     31     32614  21169  22712  33923  32543\n",
       " 20534  50409  50409   6788  10555  …  18357   7645  44842  49788  18357\n",
       " 31048  34954  37252     31   5457     21756  36048  51941  30211  52770\n",
       " 25593  30694  41037  19524    903     39526   1254  50409  24344     31\n",
       " 35399  53889  50187   5487  16415     50034  52056  53975  47951  34833\n",
       " 19524  39295  38572  17265  17960     38707  49820  39843  39526  47531\n",
       " 49300  19524  49518  16857     31  …  39659  38637     31  33053  49765\n",
       " 18357  35072  53890    903  16415     32361  30694  35409  19498  26311\n",
       " 30989  20997  16415  16415  15628        31  50034  49826  38154  32763\n",
       " 48173  46732  32966   5434     31     36365  31292  39537  29848  45091\n",
       " 53431  43973  28566     31  19524       135    135  49820     31    135"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000-element reshape(::LinearAlgebra.Adjoint{Int64,Array{Int64,2}}, 35000) with eltype Int64:\n",
       " 1943\n",
       " 3963\n",
       " 4274\n",
       " 7350\n",
       " 5620\n",
       "  128\n",
       " 2048\n",
       " 6961\n",
       " 7518\n",
       " 7828\n",
       " 2744\n",
       "    1\n",
       " 1081\n",
       "    ⋮\n",
       " 7421\n",
       " 7350\n",
       " 3927\n",
       " 6194\n",
       " 6627\n",
       " 7242\n",
       " 2518\n",
       "    2\n",
       " 4645\n",
       " 3839\n",
       "    2\n",
       " 7952"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(x, pad_size*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching size(::EmbeddingLayer)\nClosest candidates are:\n  size(!Matched::BitArray{1}) at bitarray.jl:77\n  size(!Matched::BitArray{1}, !Matched::Integer) at bitarray.jl:81\n  size(!Matched::Core.Compiler.StmtRange) at show.jl:1598\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching size(::EmbeddingLayer)\nClosest candidates are:\n  size(!Matched::BitArray{1}) at bitarray.jl:77\n  size(!Matched::BitArray{1}, !Matched::Integer) at bitarray.jl:81\n  size(!Matched::Core.Compiler.StmtRange) at show.jl:1598\n  ...",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[9]:1"
     ]
    }
   ],
   "source": [
    "EmbeddingLayer(max_features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataType"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(EmbeddingLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train,2)*70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
