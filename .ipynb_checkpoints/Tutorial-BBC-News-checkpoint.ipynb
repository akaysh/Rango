{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling TextAnalysis [a2db99b7-8b79-58f8-94bf-bbc811eef33d]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Info: Precompiling PyPlot [d330b81b-6aea-500a-939a-2ce795aea3ee]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Info: Precompiling MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d]\n",
      "└ @ Base loading.jl:1273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ADAM(0.05, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using  Languages, TextAnalysis, Flux, PyPlot, Statistics, MLDataUtils, Embeddings\n",
    "\n",
    "# function to return the index of the word in the word dictionary\n",
    "tk_idx(s) = haskey(word_dict, s) ? i=word_dict[s] : i=0\n",
    "\n",
    "# Padding the corpus wrt the longest document\n",
    "function pad_corpus(c, pad_size)\n",
    "    M=[]\n",
    "    for doc in 1:length(c)\n",
    "        tks = tokens(c[doc])\n",
    "        if length(tks)>=pad_size\n",
    "            tk_indexes=[tk_idx(w) for w in tks[1:pad_size]]\n",
    "        end\n",
    "        if length(tks)<pad_size\n",
    "            tk_indexes=zeros(Int64,pad_size-length(tks))\n",
    "            tk_indexes=vcat(tk_indexes, [tk_idx(w) for w in tks])\n",
    "        end\n",
    "        doc==1 ? M=tk_indexes' : M=vcat(M, tk_indexes')\n",
    "    end\n",
    "    return M\n",
    "end\n",
    "\n",
    "accuracy(x, y) = mean(x .== y)\n",
    "\n",
    "loss(x, y) = sum(Flux.binarycrossentropy.(m(x), y))\n",
    "\n",
    "map_binary_encoding(labels) = [label == \"__label__1\" ? 0 : 1 for label in labels ] \n",
    "optimizer = opt = ADAM(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/amazon_reviews.txt\"\n",
    "τ = 500 # max Iterations\n",
    "# M = 50 # max_features\n",
    "\n",
    "f = open(data_path)\n",
    "doc_array = readlines(f)[1:500]\n",
    "\n",
    "labels, texts = [], []\n",
    "for doc in doc_array\n",
    "    content = split(doc)\n",
    "    push!(labels,content[1])\n",
    "    push!(texts,join(content[2:end],\" \"))\n",
    "end\n",
    "\n",
    "# pushing the text from the files to the string documents\n",
    "docs=[]\n",
    "for i in 1:length(texts)\n",
    "    push!(docs, StringDocument(texts[i]))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant embtable\n",
      "WARNING: redefining constant get_word_index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "get_embedding (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const embtable = load_embeddings(GloVe{:en},3) # or load_embeddings(FastText_Text) or ...\n",
    "#Function to return the index of the word in the embedding (returns 0 if the word is not found)\n",
    "const get_word_index = Dict(word=>ii for (ii,word) in enumerate(embtable.vocab))\n",
    "\n",
    "function get_embedding(word)\n",
    "    ind = get_word_index[word]\n",
    "    emb = embtable.embeddings[:,ind]\n",
    "    return emb\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 400000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embtable.embeddings\n",
    "vocab = embtable.vocab\n",
    "embed_size, max_features = size(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building Flux Embeddings\n",
    "max_features = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8063"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a Corpus\n",
    "corpus=Corpus(docs)\n",
    "\n",
    "# updating the lexicon and creating the word dict\n",
    "update_lexicon!(corpus)\n",
    "doc_term_matrix=DocumentTermMatrix(corpus)\n",
    "word_dict = doc_term_matrix.column_indices\n",
    "# splitting words in the document\n",
    "word_docs = map(s -> split(s,r\"[,. ]\",keepempty=false),texts)\n",
    "# pad size is the number of words in the maximum word document\n",
    "# Can set a fixed length or the max doc length\n",
    "# pad_size = maximum(length(word_docs[i]) for i in 1:length(texts)) \n",
    "pad_size = 5\n",
    "# padding the docs\n",
    "padded_docs = pad_corpus(corpus, pad_size)\n",
    "# forming the data with the labels\n",
    "x = padded_docs'\n",
    "y = map_binary_encoding(labels)\n",
    "data = [(x, y)]\n",
    "\n",
    "# Building Flux Embeddings\n",
    "N = size(padded_docs,1)  #Number of documents\n",
    "# features per word to learn, depends on the size of the corpus, larger corpus will probably need a higher dimension\n",
    "# max_features = M\n",
    "# number of words in the vocabulary, should always be higher than the maximum index in our dictionary.\n",
    "ν = maximum(word_dict)[2] + 1\n",
    "vocab_size = ν"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=Flux.glorot_normal(max_features, vocab_size)\n",
    "\n",
    "function vec_idx(s)\n",
    "    i=findfirst(x -> x==s, vocab)\n",
    "    i==nothing ? i=0 : i \n",
    "end\n",
    "\n",
    "for term in doc_term_matrix.terms\n",
    "    if vec_idx(term)!=0\n",
    "        embedding_matrix[:,word_dict[term]+1]=get_embedding(term)\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511667.3 0.49412\n",
      "271282.0 0.494312\n",
      "240539.44 0.505544\n",
      "341568.88 0.505832\n",
      "288060.3 0.505784\n",
      "188451.64 0.504224\n",
      "219217.62 0.49448\n",
      "274076.56 0.494288\n",
      "249651.9 0.494336\n",
      "189548.7 0.495608\n",
      "190446.5 0.505256\n",
      "231436.03 0.505712\n",
      "231554.95 0.505712\n",
      "194984.1 0.505448\n",
      "177951.2 0.497672\n",
      "202784.88 0.49436\n",
      "214996.75 0.494312\n",
      "195465.36 0.49448\n",
      "176498.28 0.499064\n",
      "187605.66 0.505496\n",
      "201259.0 0.505736\n",
      "192641.28 0.505664\n",
      "177095.52 0.503768\n",
      "180446.47 0.495296\n",
      "191549.31 0.49448\n",
      "188113.36 0.494504\n",
      "176994.94 0.496352\n",
      "177863.28 0.504248\n",
      "185630.16 0.505496\n",
      "183855.78 0.50552\n",
      "176164.38 0.503984\n",
      "176709.19 0.4964\n",
      "182119.61 0.494744\n",
      "180494.78 0.494792\n",
      "175262.92 0.497336\n",
      "176358.27 0.504344\n",
      "179839.1 0.505256\n",
      "177879.97 0.50492\n",
      "174636.6 0.501728\n",
      "176262.9 0.495968\n",
      "178129.86 0.49508\n",
      "175960.55 0.496088\n",
      "174455.44 0.501464\n",
      "176210.03 0.504752\n",
      "176618.28 0.504896\n",
      "174731.78 0.503264\n",
      "174632.34 0.497264\n",
      "175936.47 0.495512\n",
      "175310.88 0.49592\n",
      "174191.33 0.500096\n",
      "174880.47 0.504272\n",
      "175328.81 0.504608\n",
      "174389.55 0.503192\n",
      "174215.72 0.498104\n",
      "174893.3 0.496136\n",
      "174576.9 0.496592\n",
      "174014.4 0.500624\n",
      "174400.77 0.503984\n",
      "174546.16 0.504344\n",
      "174039.3 0.502496\n",
      "174070.81 0.49832\n",
      "174363.27 0.496688\n",
      "174075.72 0.497792\n",
      "173905.02 0.501728\n",
      "174154.56 0.503648\n",
      "174066.75 0.503504\n",
      "173837.31 0.501032\n",
      "173978.34 0.49784\n",
      "174010.1 0.497624\n",
      "173810.61 0.5\n",
      "173855.78 0.502712\n",
      "173932.31 0.503456\n",
      "173789.69 0.50204\n",
      "173775.0 0.4994\n",
      "173853.2 0.498056\n",
      "173762.86 0.499208\n",
      "173722.5 0.501752\n",
      "173785.6 0.503024\n",
      "173731.03 0.5024\n",
      "173683.53 0.500576\n",
      "173729.88 0.498944\n",
      "173698.34 0.499424\n",
      "173653.62 0.501392\n",
      "173685.03 0.502664\n",
      "173666.19 0.50228\n",
      "173628.25 0.501032\n",
      "173648.95 0.49952\n",
      "173636.38 0.499688\n",
      "173605.84 0.501224\n",
      "173619.31 0.502256\n",
      "173609.19 0.502232\n",
      "173585.38 0.501104\n",
      "173594.47 0.500024\n",
      "173585.3 0.50012\n",
      "173566.81 0.501272\n",
      "173572.83 0.502208\n",
      "173563.98 0.501992\n",
      "173550.08 0.50108\n",
      "173553.84 0.500144\n",
      "173545.06 0.500384\n",
      "Bool[1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0]0.500384"
     ]
    }
   ],
   "source": [
    "# Enabling Flux\n",
    "\n",
    "\n",
    "\n",
    "m = Chain(x -> embedding_matrix * Flux.onehotbatch(reshape(x, pad_size*N), 0:vocab_size-1),\n",
    "    x -> reshape(x, max_features, pad_size, N),\n",
    "    x -> sum(x, dims=2),\n",
    "    x -> reshape(x, max_features, N),\n",
    "    Dense(max_features,1,σ)\n",
    ")\n",
    "\n",
    "loss_h=[]\n",
    "accuracy_train=[]\n",
    "\n",
    "for epoch in 1:100\n",
    "    Flux.train!(loss, Flux.params(m), data, optimizer)\n",
    "    println(loss(x, y), \" \", accuracy(m(x).>0.5,y))\n",
    "    push!(loss_h, loss(x, y))\n",
    "    push!(accuracy_train, accuracy(m(x).>0.5,y))\n",
    "end\n",
    "\n",
    "print(m(x).>0.5, accuracy(m(x).>0.5,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×1 LinearAlgebra.Adjoint{Float32,Array{Float32,2}}:\n",
       " 0.50189704\n",
       " 0.5186824 \n",
       " 0.500922  \n",
       " 0.49305755\n",
       " 0.49638537\n",
       " 0.49440265\n",
       " 0.49492964\n",
       " 0.50299513\n",
       " 0.5069653 \n",
       " 0.50857854\n",
       " 0.491336  \n",
       " 0.5063892 \n",
       " 0.4781044 \n",
       " ⋮         \n",
       " 0.5273178 \n",
       " 0.5022925 \n",
       " 0.51441485\n",
       " 0.5055788 \n",
       " 0.5116832 \n",
       " 0.5591339 \n",
       " 0.50550133\n",
       " 0.4748863 \n",
       " 0.4912404 \n",
       " 0.5156409 \n",
       " 0.4975297 \n",
       " 0.48050123"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2607×17500 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  1  0  1  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " ⋮              ⋮              ⋮        ⋱        ⋮              ⋮            \n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.onehotbatch(reshape(x, pad_size*N), 0:vocab_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 2606 entries:\n",
       "  \"1\"           => 20\n",
       "  \"Bill\"        => 123\n",
       "  \"neither.\"    => 1740\n",
       "  \"Bateman\"     => 118\n",
       "  \"Cape\"        => 146\n",
       "  \"doctor\"      => 1073\n",
       "  \"enjoy\"       => 1124\n",
       "  \"chocolate\"   => 882\n",
       "  \"fight\"       => 1225\n",
       "  \"spent\"       => 2259\n",
       "  \"regular\"     => 2021\n",
       "  \"culture.\"    => 999\n",
       "  \"artisan\"     => 703\n",
       "  \"favorites\"   => 1209\n",
       "  \"frustrating\" => 1285\n",
       "  \"loosely\"     => 1618\n",
       "  \"haze\"        => 1386\n",
       "  \"par.\"        => 1840\n",
       "  \"step\"        => 2278\n",
       "  \"Many\"        => 340\n",
       "  \"download\"    => 1088\n",
       "  \"gives\"       => 1321\n",
       "  \"irrelevant\"  => 1508\n",
       "  \"lean\"        => 1572\n",
       "  \"poised\"      => 1912\n",
       "  ⋮             => ⋮"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching labelenc(::MLLabelUtils.LabelEnc.NativeLabels{Any,2,typeof(identity)}, ::Type{MLLabelUtils.LabelEnc.TrueFalse})\nClosest candidates are:\n  labelenc(::Any) at /Users/asharma19/.julia/packages/MLLabelUtils/g0wUZ/src/labelencoding.jl:385",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching labelenc(::MLLabelUtils.LabelEnc.NativeLabels{Any,2,typeof(identity)}, ::Type{MLLabelUtils.LabelEnc.TrueFalse})\nClosest candidates are:\n  labelenc(::Any) at /Users/asharma19/.julia/packages/MLLabelUtils/g0wUZ/src/labelencoding.jl:385",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[11]:1"
     ]
    }
   ],
   "source": [
    "labelenc(labels,LabelEnc.TrueFalse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLLabelUtils.LabelEnc.NativeLabels{Any,2,typeof(identity)}(identity, Any[\"__label__2\", \"__label__1\"], Dict{Any,Int64}(\"__label__2\" => 1,\"__label__1\" => 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
